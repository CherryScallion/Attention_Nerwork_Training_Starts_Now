#In this script, we will parse the attention data from a text file and convert it into input and output matrices.
#And then we will create a classical attention mechanism function ğ´(ğ‘,ğ¾,ğ‘‰) = Î£ğ‘(ğ‘(ğ‘˜ğ‘–,ğ‘)) Ã— ğ‘£ğ‘–
#And then we train this simple neural network using this attention mechanism.
import numpy as np
import torch
import re
import numpy as np
import os
import torch
import torch.nn as nn
script_dir = os.path.dirname(os.path.abspath(__file__))
training_dir = os.path.join(script_dir, "TrainingData.txt")
def parse_attention_data(data_string):
    vector_pattern = re.compile(r'\[(-?\d+),\s*(-?\d+)\]')# Regular expression to match vectors like [x, y](this is excusive for this specific dataset we created)
    matches = vector_pattern.findall(data_string)# Find all matches in the input string
    vectors = [list(map(float, match)) for match in matches]# Convert matched strings to float lists
    input_vectors = []
    output_vectors = []
    for i in range(0, len(vectors), 10):
        input_vectors.extend(vectors[i:i+5])
        output_vectors.extend(vectors[i+5:i+10])#A smart way to split the input and output vectors
    input_matrices = np.array(input_vectors).reshape(-1, 5, 2)
    output_matrices = np.array(output_vectors).reshape(-1, 5, 2)
    return input_matrices, output_matrices
with open(training_dir, 'r') as file:
    data = file.read()
input_data, output_data = parse_attention_data(data)

# print the shapes and first few matrices to verify
print("Input shapes:", input_data.shape)
print("The first input matrix:\n", input_data[0])
print("\nOut put matrix", output_data.shape)
print("The first output matrix:\n", output_data[0])
# Now, let's define the classical attention mechanism function ğ´(ğ‘,ğ¾,ğ‘‰) = Î£ ğ‘(ğ‘(ğ‘˜ğ‘–,ğ‘)) Ã— ğ‘£ğ‘–
# 
d_model = 2

# å®šä¹‰ Query, Key, Value çš„ç»´åº¦ï¼ˆd_k, d_vï¼‰
# é€šå¸¸ä¸ºäº†ç®€åŒ–ï¼Œå®ƒä»¬ä¸ d_model ç›¸åŒ
d_k = 2
d_v = 2

# åˆ›å»ºä¸‰ä¸ªå¯è®­ç»ƒçš„çº¿æ€§å±‚ï¼Œå®ƒä»¬å°±æ˜¯ Wq, Wk, Wv çŸ©é˜µ
# nn.Linear(in_features, out_features, bias=True)
# in_features æ˜¯è¾“å…¥å‘é‡çš„ç»´åº¦
# out_features æ˜¯è¾“å‡ºå‘é‡çš„ç»´åº¦
W_q = nn.Linear(d_model, d_k, bias=False)  # Query çŸ©é˜µ
W_k = nn.Linear(d_model, d_k, bias=False)  # Key çŸ©é˜µ
W_v = nn.Linear(d_model, d_v, bias=False)  # Value çŸ©é˜µ

# æ£€æŸ¥åˆ›å»ºçš„çŸ©é˜µï¼ˆçº¿æ€§å±‚çš„æƒé‡ï¼‰
print("W_q çš„æƒé‡çŸ©é˜µå½¢çŠ¶:", W_q.weight.shape)
print("W_k çš„æƒé‡çŸ©é˜µå½¢çŠ¶:", W_k.weight.shape)
print("W_v çš„æƒé‡çŸ©é˜µå½¢çŠ¶:", W_v.weight.shape)

class Classic_Attention_Module:
    def __init__(self):
        pass
    def 
    def attention(self, q, K, V):
        """
        è®¡ç®—ç»å…¸æ³¨æ„åŠ›æœºåˆ¶
        q: æŸ¥è¯¢å‘é‡ (2,)
        K: é”®çŸ©é˜µ (5, 2)
        V: å€¼çŸ©é˜µ (5, 2)
        """
        #  a(ki, q) = ki Â· q
        scores = torch.matmul(K, q)  # (5,)
        a = torch.dot(scores, q)
        A = torch.softmax(a, dim=0)
        # 
